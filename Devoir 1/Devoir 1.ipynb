{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MNIST, distance euclidienne et similarité pixel\n",
    "Dans ce document, nous allons exécuter plusieurs algorithmes d'apprentissage machine.\n",
    "Ceci sont les importations que nous aurons besoin pour le projet. Nous utiliserons SKlearn tel que indiqué par le professeur."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sparse as sparse\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "#classification\n",
    "from sklearn.neighbors import KNeighborsClassifier # k-plus proches voisins\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "#Partitionnement\n",
    "from sklearn.cluster import AgglomerativeClustering #Regroupement hiérarchique (Partitionnement binaire)\n",
    "import kmedoids\n",
    "\n",
    "#réduction de dimensionnalité\n",
    "from sklearn.decomposition import KernelPCA #ce n'est pas PCoA mais on peut l'utiliser pour que le résultat soit le même\n",
    "from sklearn.manifold import Isomap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour commencer le projet, nous allons devoir importer MNIST et standardiser les données, car certains des algorithmes que nous devons implémenter ne fonctionnent pas très bien avec des données non standardisées. Voici les algorithmes :\n",
    "\n",
    "● k-medoïde\n",
    "● Partition binaire (Regroupement hiérarchique)\n",
    "● PCoA (c'est un cas particulier de MDS)\n",
    "● Isomap\n",
    "● KNN (k-plus proches voisins)\n",
    "\n",
    "Nous allons aussi préparer les jeux de données pour la similarité de pixel dans le bloc qui suit. La mesure de similarité que nous allons utiliser pour MNIST sera la somme des pixels dans l'image. Ensuite, nous ferons la différence de cette somme avec les autres images pour avoir le niveau de similarité. Plus le chiffre sera petit, plus les images se ressembleront, et donc elles auront plus de chance d'appartenir à la même classe. La logique ici est que les images qui font partis de la même classe devrait avoir des formes similaires, ce qui implique ainsi que le nombre de pixels allumés dans l'écran sera environ le même. Un des problèmes qui saute aux yeux dès le début avec cette similarité serait l'exemple des 6 et des 9, qui ont une forme très semblable et nous pouvons présumer que le nombre de pixels qui sont allumés doivent se ressembler fortement. Cependant, nous avons confiance que la similarité de pixels sera robuste pour les autres chiffres. Nous avons aussi décidé de ne pas binariser l'image, car nous avions l'impression que nous perdions trop de précision dans les sommes d'images et que cela rendrait nos images indistinguables.\n",
    "### Avertissement\n",
    "Les matrices de similarité et euclidiennes prennent beaucoup de RAM à calculer. Les performances des algorithmes peuvent varier selon vos spécifications machine. Pour l'algorithme de Isomap, nous avons dû utiliser un jeu de données réduit, car la matrice ne rentrait pas en mémoire. Nous avons donc utilisé une matrice des 20000 premiers exemples pour l'algorithme, et vous pouvez réduire les jeux de données tel que dans l'exemple de l'Isomap si vous voulez augmenter la rapidité des autres algorithmes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, normalize, binarize\n",
    "from sklearn.decomposition import PCA\n",
    "# https://notebook.community/francesco-mannella/neunet-basics/course/mnist\n",
    "from mnist import MNIST\n",
    "\n",
    "# init with the 'data' dir\n",
    "mndata = MNIST('.\\data')\n",
    "\n",
    "# Load data\n",
    "(train_X, train_y) = mndata.load_training()\n",
    "data_train = pd.DataFrame(train_X)\n",
    "(test_X, test_y) = mndata.load_testing()\n",
    "data_test = pd.DataFrame(test_X)\n",
    "\n",
    "print('X_train: ' + str(np.shape(train_X)))\n",
    "print('Y_train: ' + str(np.shape(train_y)))\n",
    "print('X_test:  '  + str(np.shape(test_X)))\n",
    "print('Y_test:  '  + str(np.shape(test_y)))\n",
    "\n",
    "# Binarization of data\n",
    "# Threshold of 100 for the pixels. If they are not lit enough, they are not counted.\n",
    "#X_binarized_train = binarize(train_X, threshold=100, copy=True)\n",
    "#X_binarized_test = binarize(train_X, threshold=100, copy=True)\n",
    "\n",
    "# Pixel similarity transformation\n",
    "X_pixel_train = np.zeros(shape=(len(train_X), 1), dtype='int')\n",
    "X_pixel_test = np.zeros(shape=(len(test_X), 1), dtype='int')\n",
    "\n",
    "# Summation of all the pixels inside the training set\n",
    "for i in range(len(train_X)):\n",
    "    #X_pixel_train[i] = np.sum(X_binarized_train[i], axis=0)\n",
    "    X_pixel_train[i] = np.sum(train_X[i], axis=0)\n",
    "\n",
    "# Summation of all the pixels inside the testing set\n",
    "for i in range(len(test_X)):\n",
    "    #X_pixel_test[i] = np.sum(X_binarized_test[i], axis=0)\n",
    "    X_pixel_test[i] = np.sum(test_X[i], axis=0)\n",
    "\n",
    "# Standardizing the data (preprocessing)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_std_train = scaler.fit_transform(train_X)\n",
    "X_std_pixel_train = scaler.fit_transform(X_pixel_train)\n",
    "\n",
    "X_std_test = scaler.fit_transform(test_X)\n",
    "X_std_pixel_test = scaler.fit_transform(X_pixel_test)\n",
    "\n",
    "# Reduction of dimensionality\n",
    "# This step is necessary for some algorithms i.e. k-medoids is just like kmeans and the algorithm doesn't scale very well to high dimensions\n",
    "pca = PCA(n_components = 2)\n",
    "# Reduced training data set\n",
    "X_principal = pca.fit_transform(X_std_train)\n",
    "cutoff = np.median(X_principal)\n",
    "\n",
    "# The number of pixels per side of all images\n",
    "img_side = 28\n",
    "\n",
    "# Each input is a raw vector.\n",
    "# The number of units of the network\n",
    "# corresponds to the number of input elements\n",
    "n_mnist_pixels = img_side*img_side"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ceci est la méthode qui sera utilisée pour générer la matrix de similarité"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pixel_similarity(x, y):\n",
    "    # return the pixel similarity matrix\n",
    "    return np.abs(x-y).sum(axis=2)\n",
    "\n",
    "\n",
    "def get_pixel_similarity_matrix(X, Y=None):\n",
    "    Y = X if Y is None else Y\n",
    "    return pixel_similarity(X[:,None], Y[None,:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cette partie ici ne sert qu'à tester sur votre machine combien de temps de calcul la machine prendra en moyenne pour calculer la matrix de similarité. Nous avons inclus en commentaires le temps que cela a pris sur notre machine."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "print(np.shape(get_pixel_similarity_matrix(X_std_pixel_train)))\n",
    "print(\"Calculating the train pixel similarity matrix took: %.2f s\" % (time() - start))\n",
    "# Calculating the test pixel similarity matrix took: 271.51 s (5 minutes)\n",
    "# Calculating the train pixel similarity matrix took: 704.23 s (12 minutes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ici on vérifie que le dataset à bien téléchargé et nous imprimons quelques images pour s'assurer que tout est en ordre."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pylab import *\n",
    "\n",
    "# Define the number of samples to take\n",
    "num_samples = 10\n",
    "\n",
    "# create a figure where we will store all samples\n",
    "figure(figsize=(10,1))\n",
    "\n",
    "# Iterate over samples indices\n",
    "for sample in range(num_samples) :\n",
    "\n",
    "    # The image corresponding to the 'sample' index\n",
    "    img = train_X[sample]\n",
    "\n",
    "    # The label of the image\n",
    "    label = train_y[sample]\n",
    "\n",
    "    # The image is stored as a rolled vector,\n",
    "    # we have to roll it back in a matrix\n",
    "    aimg = array(img).reshape(img_side, img_side)\n",
    "\n",
    "    # Open a subplot for each sample\n",
    "    subplot(1, num_samples, sample+1)\n",
    "\n",
    "    # The corresponding digit is the title of the plot\n",
    "    title(label)\n",
    "\n",
    "    # We use imshow to plot the matrix of pixels\n",
    "    imshow(aimg, interpolation = 'none',\n",
    "        aspect = 'auto', cmap = cm.binary)\n",
    "    axis(\"off\")\n",
    "\n",
    "show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Algorithme de K-medoïde\n",
    "## Distance Euclidienne\n",
    "Nous allons commencer par faire l'algorithme de k-medoïde avec la distance euclidienne pour MNIST. Dans le bloc suivant, il y aura le même algorithme, mais avec la similarité de pixel. Cet algorithme ne passe pas très bien à l'échelle pour les données de grande dimension, donc nous devons utiliser une réduction de dimensionalité pour la distance euclidienne, car 784 dimensions est beaucoup trop grand pour cet algorithme et il ne se terminait jamais sur notre machine, donc nous l'avons réduit à 2 dimensions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html?fbclid=IwAR2CjekTcfhnp19Txryk2u5c2w2Zoa9O1kEBR710Bcg8kPNuCXQWj0-_zm8\n",
    "# https://medium.com/@ali.soleymani.co/beyond-scikit-learn-is-it-time-to-retire-k-means-and-use-this-method-instead-b8eb9ca9079a\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "fp = kmedoids.fasterpam(euclidean_distances(X_principal), medoids=10, max_iter=100)\n",
    "\n",
    "print(\"FasterPAM with euclidean distance took: %.2f ms\" % ((time() - start)*1000))\n",
    "print(\"Loss with FasterPAM:\", fp.loss)\n",
    "homo = metrics.homogeneity_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "compl = metrics.completeness_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "v_meas = metrics.v_measure_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "ARI = metrics.adjusted_rand_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "AMI = metrics.adjusted_mutual_info_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "silhouette = metrics.silhouette_score(X_principal,train_y, metric='euclidean', sample_size=np.shape(X_principal)[0])\n",
    "\n",
    "print(82 * \"_\")\n",
    "print(\"\\ttime\\thomo\\tcompl\\tv_meas\\tARI\\tAMI\\tsilhouette\")\n",
    "print(f\"{(time() - start)*1000} ms {homo} {compl} {v_meas} {ARI} {AMI} {silhouette}\")\n",
    "print(82 * \"_\")\n",
    "\n",
    "# Commented this section because it takes way too much time, for very similar results, partly due to the deterministic nature of PCA\n",
    "# start = time()\n",
    "# pam = kmedoids.pam(diss, medoids=10, max_iter=100)\n",
    "# homo = metrics.homogeneity_score(labels_true=train_y, labels_pred=pam.labels)\n",
    "# compl = metrics.completeness_score(labels_true=train_y, labels_pred=pam.labels)\n",
    "# v_meas = metrics.v_measure_score(labels_true=train_y, labels_pred=pam.labels)\n",
    "# ARI = metrics.adjusted_rand_score(labels_true=train_y, labels_pred=pam.labels)\n",
    "# AMI = metrics.adjusted_mutual_info_score(labels_true=train_y, labels_pred=pam.labels)\n",
    "# silhouette = metrics.silhouette_score(X_principal,train_y, metric='euclidean', sample_size=np.shape(X_principal)[0])\n",
    "# print(\"Loss with PAM:\", pam.loss)\n",
    "# print(\"PAM took: %.2f ms\" % ((time() - start)*1000))\n",
    "# print(82 * \"_\")\n",
    "# print(\"\\ttime\\thomo\\tcompl\\tv_meas\\tARI\\tAMI\\tsilhouette\")\n",
    "# print(f\"{(time() - start)*1000} ms {homo} {compl} {v_meas} {ARI} {AMI} {silhouette}\")\n",
    "# print(82 * \"_\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarité de Pixel\n",
    "Pas besoin de réduire la dimension, car la similarité de pixel réduit déjà à une seule dimension."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "fp = kmedoids.fasterpam(get_pixel_similarity_matrix(X_std_pixel_train), medoids=10, max_iter=100)\n",
    "\n",
    "print(\"FasterPAM with pixel similarity took: %.2f ms\" % ((time() - start)*1000))\n",
    "print(\"Loss with FasterPAM:\", fp.loss)\n",
    "homo = metrics.homogeneity_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "compl = metrics.completeness_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "v_meas = metrics.v_measure_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "ARI = metrics.adjusted_rand_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "AMI = metrics.adjusted_mutual_info_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "silhouette = metrics.silhouette_score(get_pixel_similarity_matrix(X_std_pixel_train),train_y, metric='precomputed', sample_size=np.shape(X_std_pixel_train)[0])\n",
    "\n",
    "print(82 * \"_\")\n",
    "print(\"\\ttime\\thomo\\tcompl\\tv_meas\\tARI\\tAMI\\tsilhouette\")\n",
    "print(f\"{(time() - start)*1000} ms {homo} {compl} {v_meas} {ARI} {AMI} {silhouette}\")\n",
    "print(82 * \"_\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "Voici les métriques collectées pour la distance euclidienne :\n",
    "__________________________________________________________________________________\n",
    "\ttime(s) homo    compl   v_meas  ARI     AMI     silhouette\n",
    "    620.091 0.2175  0.2234  0.2204  0.1427  0.2202  -0.0670\n",
    "__________________________________________________________________________________\n",
    "\n",
    "Et voici les métriques collectées pour la similarité de pixel :\n",
    "__________________________________________________________________________________\n",
    "\ttime(s) homo    compl   v_meas  ARI     AMI     silhouette\n",
    "\t1407.364 0.1017 0.1037  0.1027  0.0527  0.1024  -0.1337\n",
    "__________________________________________________________________________________\n",
    "\n",
    "Il est assez clair ici avec ces métriques, que la similarité de pixel est beaucoup plus lente, environ 2x plus lente et les groupes sont beaucoup moins homogènes. En général, les métriques sont beaucoup moins bonnes pour la similarité, on peut donc en conclure que ce n'est pas une bonne métrique pour l'algorithme des K-medoïde. En termes de temps, la distance euclidienne prend moins de temps, mais cela est en partie parce que nous avons déjà fait la réduction de dimensionalité au préalable, ce qui accélère le calcul. Bien sûr, même avec la réduction de dimensionalité, le temps total sera moins long que la similarité, mais ils deviendront plus proches pour cette métrique. Donc les résultats sont moins bons que la distance euclidienne, mais pour le temps de calcul, cela est similaire si on prend en compte le temps de réduction de dimensionalité qui a été faite au préalable."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Partionnement Binaire\n",
    "## Distance Euclidienne\n",
    "Ici nous ferons la partition binaire avec la distance euclidienne pour MNIST. Pour la distance euclidienne et la similarité, nous avons assumé qu'il y avait 10 groupes, car nous savons qu'il y a 10 classes d'images pour MNIST. Nous avions tenté de normaliser les données, mais nous avons remarqué que c'est une mauvaise idée, parce que les groupes se mélangent et perde leurs identités lorsqu'ils sont normaux."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "# https://www.projectpro.io/recipes/do-agglomerative-clustering-in-python\n",
    "# https://www.geeksforgeeks.org/implementing-agglomerative-clustering-using-sklearn/\n",
    "\n",
    "# pca = PCA(n_components = 2)\n",
    "# # Training data set\n",
    "# X_principal = pca.fit_transform(X_std_train)\n",
    "# X_principal = pd.DataFrame(X_principal)\n",
    "# print(X_principal)\n",
    "#X_principal.columns = ['P1', 'P2']\n",
    "\n",
    "# # Testing data set\n",
    "# X_secondary = pca.fit_transform(X_normalized_test)\n",
    "# X_secondary = pd.DataFrame(X_secondary)\n",
    "# X_secondary.columns = ['P1', 'P2']\n",
    "\n",
    "# Dendogram requires normalized data, which is bad for mnist\n",
    "# plt.figure(figsize =(8, 8))\n",
    "# plt.title('Visualising the data')\n",
    "# Dendrogram = shc.dendrogram((shc.linkage(X_principal, method ='ward')))\n",
    "\n",
    "\n",
    "start = time()\n",
    "ac10 = AgglomerativeClustering(n_clusters = 10, affinity='euclidean')\n",
    "labels = pd.DataFrame(ac10.fit_predict(X_std_train))\n",
    "\n",
    "#vscore = v_measure_score(labels_true=train_y, labels_pred=labels.iloc[:, 0])\n",
    "\n",
    "#print(f\"V-measure score for agglomerative clustering with 10 clusters: {vscore}\")\n",
    "\n",
    "print(\"Agglomerative clustering with euclidean distance took: %.2f s\" % (time() - start))\n",
    "print(f\"V-measure score for agglomerative clustering with 10 clusters: {v_measure_score(labels_true=train_y, labels_pred=labels.iloc[:, 0])}\")\n",
    "\n",
    "# homo = metrics.homogeneity_score(labels_true=train_y, labels_pred=labels.iloc[:, 0])\n",
    "# compl = metrics.completeness_score(labels_true=train_y, labels_pred=labels.iloc[:, 0])\n",
    "# v_meas = v_measure_score(labels_true=train_y, labels_pred=labels.iloc[:, 0])\n",
    "# ARI = metrics.adjusted_rand_score(labels_true=train_y, labels_pred=labels.iloc[:, 0])\n",
    "# AMI = metrics.adjusted_mutual_info_score(labels_true=train_y, labels_pred=labels.iloc[:, 0])\n",
    "# silhouette = metrics.silhouette_score(X_std_train,train_y, metric='euclidean', sample_size=np.shape(X_std_train)[0])\n",
    "\n",
    "\n",
    "# V-measure score for agglomerative clustering with 10 clusters: 0.5844426454187285\n",
    "# Agglomerative clustering with euclidean distance took: 801.10 s\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarité de Pixel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "start = time()\n",
    "ac_pixel = AgglomerativeClustering(n_clusters = 10, affinity='precomputed', linkage='average')\n",
    "labels = ac_pixel.fit_predict(get_pixel_similarity_matrix(X_std_pixel_train))\n",
    "print(f\"V-measure score for agglomerative clustering with 10 clusters: {v_measure_score(labels_true=train_y, labels_pred=labels)}\")\n",
    "print(\"Agglomerative clustering with pixel similarity took: %.2f s\" % (time() - start))\n",
    "# Agglomerative clustering with pixel similarity took: 12135.04 s (202.25 minutes / 3.37 hours)\n",
    "# V-measure score for agglomerative clustering with 10 clusters: 0.1076184112661532"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "Puisque nous avons accès aux étiquettes pour les données, nous pouvons utiliser le v_measure_score pour évaluer la précision de l'algorithme.\n",
    "Voici les résultats pour la distance euclidienne :\n",
    "__Agglomerative clustering with euclidean distance took: 801.10 s__\n",
    "__V-measure score for agglomerative clustering with 10 clusters : 0.5844426454187285__\n",
    "\n",
    "Et les résultats pour la similarité de Pixel :\n",
    "__Agglomerative clustering with pixel similarity took: 12135.04 s (202.25 minutes / 3.37 hours)__\n",
    "__V-measure score for agglomerative clustering with 10 clusters: 0.1076184112661532__\n",
    "\n",
    "Le résultat est assez évident que la similarité de pixel n'est pas du tout capable de faire une séparation claire. C'est à peine meilleur que de tirer au hasard, ce qui à vrai dire, n'est pas utile. En termes de temps, l'algorithme avec la similarité prend plus de temps que celle avec la distance euclidienne et de beaucoup. Pour l'algorithme de partionnement, la similarité est pire que la distance euclidienne en tout points. Peut-être que si notre similarité avait des distinctions remarquables entre les classes, cela serait plus facile de faire des classes qui sépare bien. Sûrement que la binarization serait bénéfique ici, car cela nous permettrait de faire une distinction plus prononcée des classes, parce qu'on réduirait aux pixels les plus significatifs seulement et cela ferait sûrement un accent plus prononcé sur les caractéristiques importantes de chacun des groupes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCoA\n",
    "## Distance Euclidienne\n",
    "Ici nous ferons le PCoA avec la distance euclidienne pour MNIST"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "pcoa = KernelPCA(n_components=1, kernel='precomputed')\n",
    "\n",
    "# Euclidean distance\n",
    "start = time()\n",
    "pcoa_euclidean_train = pcoa.fit_transform(-.5 * euclidean_distances(X_std_train) ** 2) #-.5*D**2 est crucial!!!\n",
    "print(\"PCoA training with euclidean distance took: %.2f s\" % (time() - start))\n",
    "start = time()\n",
    "pcoa_euclidean_test = pcoa.transform(-.5 * euclidean_distances(X_std_test, X_std_train) ** 2) #-.5*D**2 est crucial!!!\n",
    "print(\"PCoA testing with euclidean distance took: %.2f s\" % (time() - start))\n",
    "\n",
    "\n",
    "# Euclidean distance\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(211)\n",
    "ax.set_title('PCoA sur train')\n",
    "ax.scatter(pcoa_euclidean_train, np.zeros_like(pcoa_euclidean_train))\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.set_title('PCoA sur test')\n",
    "ax.scatter(pcoa_euclidean_test, np.zeros_like(pcoa_euclidean_test))\n",
    "#PCoA training with euclidean distance took: 5392.98 s\n",
    "\n",
    "#PCoA training with euclidean distance took: 2405.75 s\n",
    "#PCoA testing with euclidean distance took: 389.59 s\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarité de Pixel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "pcoa = KernelPCA(n_components=1, kernel='precomputed')\n",
    "\n",
    "# Pixel similarity\n",
    "start = time()\n",
    "pcoa_pixel_train = pcoa.fit_transform(-.5 * get_pixel_similarity_matrix(X_std_pixel_train) ** 2) #-.5*D**2 est crucial!!!\n",
    "print(\"PCoA training with pixel similarity took: %.2f s\" % (time() - start))\n",
    "# PCoA training with pixel similarity took: 2932.46 s (49 minutes)\n",
    "\n",
    "start = time()\n",
    "pcoa_pixel_test = pcoa.transform(-.5 * get_pixel_similarity_matrix(X_std_pixel_test, X_std_pixel_train) ** 2) #-.5*D**2 est crucial!!!\n",
    "print(\"PCoA testing with pixel similarity took: %.2f s\" % (time() - start))\n",
    "# PCoA training with pixel similarity took: 2535.06 s\n",
    "# PCoA testing with pixel similarity took: 343.85 s\n",
    "\n",
    "# Pixel similarity\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(211)\n",
    "ax.set_title('PCoA sur train')\n",
    "ax.scatter(pcoa_pixel_train, np.zeros_like(pcoa_pixel_train))\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.set_title('PCoA sur test')\n",
    "ax.scatter(pcoa_pixel_test, np.zeros_like(pcoa_pixel_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "Voici le résultat que nous avons obtenu pour la distance euclidienne :\n",
    "![Euclidean Distance](Rapport/pcoa_euclidean.png)\n",
    "__PCoA training with euclidean distance took: 2405.75 s__\n",
    "__PCoA testing with euclidean distance took: 389.59 s__\n",
    "__Total: 2,795.34 s (46.5 m)__\n",
    "\n",
    "Et voici le résultat de la similarité de pixel :\n",
    "![Similarité Pixel](Rapport/pcoa_pixel_test.png)\n",
    "__PCoA training with pixel similarity took: 2535.06 s__\n",
    "__PCoA testing with pixel similarity took: 343.85 s__\n",
    "__Total: 2,878.91 s (48 m)__\n",
    "\n",
    "Puisque le PCoA est une transformation sur le plan à 1 dimension, et que la similarité de pixel est déjà réduite à une dimension, il aurait été logique que la transformation soit plus rapide. Cependant, les deux on prit un temps très similaire. Parce que le calcul de la similarité de pixel inclus une réduction de dimensionalité dans son calcul, cela lui permet d'être compétitif sur ce point. La distance euclidienne doit être réduite avant d'être calculé, ce qui rajoute un peu de temps de calcul. Finalement, la similarité de pixel et la distance euclidienne sont presque équivalente pour PCoA. Sur les graphiques, ont peut voir qu'il y a quelques différences sur les extrêmes de la droite, mais cela semble être mineur et ils pourraient être dû à la nature du changement des données dans la similarité."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KNN\n",
    "## Distance Euclidienne\n",
    "Ici nous ferons le KNN avec la distance euclidienne pour MNIST. Après quelques tests aléatoire, nous avons trouvé que 5 voisins était un bon hyperparamètre pour l'algorithme de KNN dans le cas de MNIST."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2, algorithm='auto')\n",
    "knn.fit(train_X, train_y)\n",
    "print(\"KNN training with euclidean distance took: %.2f s\" % (time() - start))\n",
    "\n",
    "start = time()\n",
    "predictions = knn.predict(test_X)\n",
    "print(\"KNN predictions with euclidean distance took: %.2f s\" % (time() - start))\n",
    "\n",
    "acc = accuracy_score(y_true=test_y, y_pred=predictions)\n",
    "print(f\"Accuracy for KNN algorithm with euclidean distance: {acc}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarité de Pixel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='precomputed', algorithm='auto')\n",
    "knn.fit(get_pixel_similarity_matrix(X_std_pixel_train), train_y)\n",
    "print(\"KNN training with pixel similarity took: %.2f s\" % (time() - start))\n",
    "# KNN training with pixel similarity took: 1261.69 s (21 minutes)\n",
    "\n",
    "start = time()\n",
    "predictions = knn.predict(get_pixel_similarity_matrix(X_std_pixel_test, X_std_pixel_train))\n",
    "print(\"KNN predictions with pixel similarity took: %.2f s\" % (time() - start))\n",
    "acc = accuracy_score(y_true=test_y, y_pred=predictions)\n",
    "print(f\"Accuracy for KNN algorithm with pixel similarity: {acc}\")\n",
    "# KNN training with pixel similarity took: 4880.94 s\n",
    "# KNN predictions with pixel similarity took: 726.11 s\n",
    "# Accuracy for KNN algorithm with pixel similarity: 0.1904"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "Voici nos métriques pour la distance euclidienne:\n",
    "__KNN training with euclidean distance took: 3.93 s__\n",
    "__KNN predictions with euclidean distance took: 12.42 s__\n",
    "__Accuracy for KNN algorithm with euclidean distance: 0.9688__\n",
    "\n",
    "Voici nos métriques pour la similarité de pixel:\n",
    "__KNN training with pixel similarity took: 4880.94 s__\n",
    "__KNN predictions with pixel similarity took: 726.11 s__\n",
    "__Accuracy for KNN algorithm with pixel similarity: 0.1904__\n",
    "\n",
    "Ici la différence est très flagrante. La précision de 19 % pour la similarité est très mauvais comparé à celui de la distance euclidienne de 96.8 %. La différence en temps est très énorme aussi. La similarité prend plusieurs minutes sur le jeu de données complet, tandis que la distance euclidienne retourne un résultat pratiquement instantanément. Nous croyons que la similarité est plus lente ici, car nous devons calculer la matrice des distances séparément et que SKlearn doit surement optimiser le calcul euclidien qui est intégré dans l'algorithme. Donc, la similarité de pixel est moins bonne que la distance euclidienne pour l'algorithme de KNN, mais pour le temps de calcul, nous croyons que ce n'est pas significatif pour cet algorithme, car il y a peut-être des optimisations que nous n'avons pas pris en compte."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Isomap\n",
    "## Distance Euclidienne\n",
    "Ici nous ferons l’Isomap avec la distance euclidienne pour MNIST. Pour cet algorithme, nous avons aussi testé quelques valeurs pour le nombre de voisins et nous avions trouvé que 5 donnait un bon résultat."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "isomap = Isomap(n_components=2, n_neighbors=5, metric='minkowski', p=2)\n",
    "\n",
    "start = time()\n",
    "X_transformed_isomap = isomap.fit_transform(X_std_train[0:20000, :])\n",
    "\n",
    "print(\"Isomap with euclidean distance took: %.2f s\" % (time() - start))\n",
    "\n",
    "fig = plt.figure(figsize=(24, 24))\n",
    "ax = fig.add_subplot(211)\n",
    "ax.set_title('Isomap euclidien sur MNIST')\n",
    "ax.scatter(X_transformed_isomap, np.zeros_like(X_transformed_isomap))\n",
    "\n",
    "# Isomap took: 217.12 s\n",
    "\n",
    "\n",
    "# Isomap_5n took: 6431123.26 ms (6,431.12326 s) (107.19 m) (1.79 h)\n",
    "# UserWarning: The number of connected components of the neighbors graph is 37 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
    "# self._fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarité de Pixel\n",
    "Après quelques essais aléatoires, nous avons trouvé que pour l'hyperparamètre des voisins, 12 semblait donné un résultat assez rapidement et nous n'avions plus l'erreur suivante :\n",
    "__UserWarning: The number of connected components of the neighbors graph is 14 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.__\n",
    "\n",
    "Nous avions aussi l'erreur suivante, donc nous étions obligés de réduire la taille de l'échantillon pour que cela fonctionne :\n",
    "__MemoryError: Unable to allocate 26.8 GiB for an array with shape (60000, 60000) and data type float64__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "isomap = Isomap(n_components=1, n_neighbors=12, metric='precomputed')\n",
    "start = time()\n",
    "X_transformed_isomap = isomap.fit_transform(get_pixel_similarity_matrix(X_std_pixel_train[0:20000, :]))\n",
    "\n",
    "print(\"Isomap with pixel similarity took: %.2f s\" % (time() - start))\n",
    "\n",
    "fig = plt.figure(figsize=(24, 24))\n",
    "ax = fig.add_subplot(211)\n",
    "ax.set_title('Isomap pixel sur MNIST')\n",
    "ax.scatter(X_transformed_isomap, np.zeros_like(X_transformed_isomap))\n",
    "#Isomap took: 133.88 s\n",
    "\n",
    "\n",
    "#D:\\School Books\\Session 5\\IFT 3700\\IFT-3700_TPs\\Devoir 1\\Devoir1\\lib\\site-packages\\sklearn\\manifold\\_isomap.py:348: UserWarning: The number of connected components of the neighbors graph is 14 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
    "#   self._fit_transform(X)\n",
    "# D:\\School Books\\Session 5\\IFT 3700\\IFT-3700_TPs\\Devoir 1\\Devoir1\\lib\\site-packages\\scipy\\sparse\\_index.py:103: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
    "#   self._set_intXint(row, col, x.flat[0])\n",
    "# MemoryError: Unable to allocate 26.8 GiB for an array with shape (60000, 60000) and data type float64"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Voici nos résultats pour Isomap à 5 voisins et la distance euclidienne :\n",
    "![Isomap_Euclidean](Rapport/Isomap_euclidien_5n_20k.png)\n",
    "__Isomap with euclidean distance took: 217.12 s__\n",
    "Voici nos résultats pour Isomap à 5 voisins et la similarité de pixel :\n",
    "![Isomap_Pixel](Rapport/Isomap_pixel_12n_20k.png)\n",
    "__Isomap with pixel similarity took: 133.88 s__\n",
    "\n",
    "Ici l'algorithme de Isomap à pris moins de temps pour la similarité, car les données sont déjà réduites à une seule dimension, et donc l'algorithme ne fait qu'une translation de l'image, ce qui prend beaucoup moins de temps que de faire la réduction en plus de faire les distances géodésiques. On peut voir que l'algorithme prend plus ou moins 1.5 fois plus de temps que la distance euclidienne. On peut très bien observer sur les graphiques que pour la similarité, les points sont beaucoup plus proches que la distance euclidienne. Cela peut être la raison pour laquelle certains des autres algorithmes ne sont pas capables de différencier les groupes clairement, car les images sont presque tous pareils."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Résumé\n",
    "Lors de nos observations pour les différences entre les algorithmes, nous avons remarqué que la similarité choisie était pire pour classifier que la distance euclidienne et parfois elle était similaire en performance de temps, et parfois elle était meilleur ou pire. Pour la classification d'images, la similarité de pixel n'est pas très efficace pour classifier les images correctement et elle n'est pas très bonne pour regrouper non plus, car elle élimine trop de dimension dans les images, ce qui les rends presque tous pareils. Ceci nous mène à la conclusion que la similarité de pixel est pire que la distance euclidenne pour l'évaluation d'images. Nous étions conscients de la perte d'information lors de notre compression de l'image à une seule dimension, mais nous ne pensions pas que cette transformation donnerait des si mauvais résultats. Visiblement, l'addition des pixels ne permet pas de différencier les images assez bien, surement dû au fait que les images peuvent contenir un nombre de pixels assez différent dans la même catégorie, par exemple un 1 qui est écrit avec la barre en haut et en bas à un nombre complètement différent de pixel qu'un 1 qui est écrit seulement avec la barre du milieu. Cela doit causer beaucoup trop de variance dans les catégories, ce qui doit surement confondre les catégories ensemble et nuire à la machine d'apprendre des catégories distinctes. Nous aurions surement pu binarizer nos données et cela nous aurait peut-être un peu aidé à mieux distinguer les groupes, car nous aurions seulement gardé les pixels les plus significatifs, ce qui aurait peut-être aidé la distinction des groupes. En conclusion, notre similarité choisie pour MNIST n'était pas très bonne et ne devrait pas être utilisé pour ce contexte, et encore moins pour les images en couleur si nous ne modifions rien aux calculs et à la réduction de dimensionalité. Les forces de cette approche ne sont pas vraiment révélées avec ce projet, mais on peut comprendre pourquoi la réduction de dimensionalité de l'image à une seule dimension peut être intéressant, car nous avons gagné en rapidité sur l'algorithme de Isomap et PCoA donnait un résultat très similaire à la distance euclidienne. Les images sont représentables dans un graphique lorsqu'elles sont réduites ainsi et elles sont compressées, ce qui en théorie devrait réduire l'espace mémoire requise pour faire les calculs. La plus grande faiblesse de l'algorithme est que nous perdons beaucoup trop d'informations pour pouvoir faire des groupes distincts puis la plupart des algorithmes de regroupement et de classification ne donneront pas de bons résultats à cause de cela."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}