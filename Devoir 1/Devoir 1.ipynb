{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MNIST, distance euclidienne et similarité pixel\n",
    "Dans ce document, nous allons exécuter plusieurs algorithmes d'apprentissage machine.\n",
    "Ceci sont les importations que nous aurons besoin pour le projet. Nous utiliserons SKlearn tel que indiqué par le professeur."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sparse as sparse\n",
    "from sklearn.metrics import accuracy_score\n",
    "#classification\n",
    "from sklearn.neighbors import KNeighborsClassifier #k-plus proches voisins\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "#Partitionnement\n",
    "from sklearn.cluster import AgglomerativeClustering #Regroupement hiérarchique (Partitionnement binaire)\n",
    "#from pyclustering.cluster.kmedoids import kmedoids\n",
    "import kmedoids\n",
    "\n",
    "#réduction de dimensionnalité\n",
    "from sklearn.decomposition import KernelPCA #ce n'est pas PCoA mais on peut l'utiliser pour que le résultat soit le même\n",
    "from sklearn.manifold import Isomap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour commencer le projet, nous allons devoir importer MNIST et standardiser les données, car certains des algorithmes que nous devons implémenter ne fonctionnent pas très bien avec des données non standardisées. Voici les algorithmes :\n",
    "\n",
    "● k-medoïde\n",
    "● Partition binaire (Regroupement hiérarchique)\n",
    "● PCoA (c'est un cas particulier de MDS)\n",
    "● Isomap\n",
    "● KNN (k-plus proches voisins)\n",
    "\n",
    "Nous allons aussi préparer les jeux de données pour la similarité de pixel dans le bloc qui suit. La mesure de similarité que nous allons utiliser pour MNIST sera la somme des pixels dans l'image. Ensuite, nous ferons la différence de cette somme avec les autres images pour avoir le niveau de similarité. Plus le chiffre sera petit, plus les images se ressembleront, et donc elles auront plus de chance d'appartenir à la même classe. La logique ici est que les images qui font partis de la même classe devrait avoir des formes similaires, ce qui implique ainsi que le nombre de pixels allumés dans l'écran sera environ le même. Un des problèmes qui saute aux yeux dès le début avec cette similarité serait l'exemple des 6 et des 9, qui ont une forme très semblable et nous pouvons présumer que le nombre de pixels qui sont allumés doivent se ressembler fortement. Cependant, nous avons confiance que la similarité de pixels sera robuste pour les autres chiffres. Nous avons aussi décidé de ne pas binariser l'image, car nous avions l'impression que nous perdions trop de précision dans les sommes d'images et que cela rendrait nos images indistinguables.\n",
    "### Avertissement\n",
    "Les matrices de similarité et euclidiennes prennent beaucoup de RAM à calculer. Les performances des algorithmes peuvent varier selon vos spécifications machine."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, normalize, binarize\n",
    "from sklearn.decomposition import PCA\n",
    "# https://notebook.community/francesco-mannella/neunet-basics/course/mnist\n",
    "from mnist import MNIST\n",
    "\n",
    "# init with the 'data' dir\n",
    "mndata = MNIST('.\\data')\n",
    "\n",
    "# Load data\n",
    "(train_X, train_y) = mndata.load_training()\n",
    "data_train = pd.DataFrame(train_X)\n",
    "(test_X, test_y) = mndata.load_testing()\n",
    "data_test = pd.DataFrame(test_X)\n",
    "\n",
    "print('X_train: ' + str(np.shape(train_X)))\n",
    "print('Y_train: ' + str(np.shape(train_y)))\n",
    "print('X_test:  '  + str(np.shape(test_X)))\n",
    "print('Y_test:  '  + str(np.shape(test_y)))\n",
    "\n",
    "# Binarization of data\n",
    "# Threshold of 100 for the pixels. If they are not lit enough, they are not counted.\n",
    "#X_binarized_train = binarize(train_X, threshold=100, copy=True)\n",
    "#X_binarized_test = binarize(train_X, threshold=100, copy=True)\n",
    "\n",
    "# Pixel similarity transformation\n",
    "X_pixel_train = np.zeros(shape=(len(train_X), 1), dtype='int')\n",
    "X_pixel_test = np.zeros(shape=(len(test_X), 1), dtype='int')\n",
    "\n",
    "# Summation of all the pixels inside the training set\n",
    "for i in range(len(train_X)):\n",
    "    #X_pixel_train[i] = np.sum(X_binarized_train[i], axis=0)\n",
    "    X_pixel_train[i] = np.sum(train_X[i], axis=0)\n",
    "\n",
    "# Summation of all the pixels inside the testing set\n",
    "for i in range(len(test_X)):\n",
    "    #X_pixel_test[i] = np.sum(X_binarized_test[i], axis=0)\n",
    "    X_pixel_test[i] = np.sum(test_X[i], axis=0)\n",
    "\n",
    "# Standardizing the data (preprocessing)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_std_train = scaler.fit_transform(train_X)\n",
    "X_std_pixel_train = scaler.fit_transform(X_pixel_train)\n",
    "\n",
    "X_std_test = scaler.fit_transform(test_X)\n",
    "X_std_pixel_test = scaler.fit_transform(X_pixel_test)\n",
    "\n",
    "# Reduction of dimensionality\n",
    "# This step is necessary for some algorithms i.e. k-medoids is just like kmeans and the algorithm doesn't scale very well to high dimensions\n",
    "pca = PCA(n_components = 2)\n",
    "# Reduced training data set\n",
    "X_principal = pca.fit_transform(X_std_train)\n",
    "cutoff = np.median(X_principal)\n",
    "\n",
    "# The number of pixels per side of all images\n",
    "img_side = 28\n",
    "\n",
    "# Each input is a raw vector.\n",
    "# The number of units of the network\n",
    "# corresponds to the number of input elements\n",
    "n_mnist_pixels = img_side*img_side"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ceci est la méthode qui sera utilisée pour générer la matrix de similarité"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pixel_similarity(x, y):\n",
    "    # return the pixel similarity matrix\n",
    "    return np.abs(x-y).sum(axis=2)\n",
    "\n",
    "\n",
    "def get_pixel_similarity_matrix(X, Y=None):\n",
    "    Y = X if Y is None else Y\n",
    "    return pixel_similarity(X[:,None], Y[None,:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cette partie ici ne sert qu'à tester sur votre machine combien de temps de calcul la machine prendra en moyenne pour calculer la matrix de similarité. Nous avons inclus en commentaires le temps que cela a pris sur notre machine."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "print(np.shape(get_pixel_similarity_matrix(X_std_pixel_train)))\n",
    "print(\"Calculating the train pixel similarity matrix took: %.2f s\" % (time() - start))\n",
    "# Calculating the test pixel similarity matrix took: 271.51 s (5 minutes)\n",
    "# Calculating the train pixel similarity matrix took: 704.23 s (12 minutes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ici on vérifie que le dataset à bien téléchargé et nous imprimons quelques images pour s'assurer que tout est en ordre."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pylab import *\n",
    "\n",
    "# Define the number of samples to take\n",
    "num_samples = 10\n",
    "\n",
    "# create a figure where we will store all samples\n",
    "figure(figsize=(10,1))\n",
    "\n",
    "# Iterate over samples indices\n",
    "for sample in range(num_samples) :\n",
    "\n",
    "    # The image corresponding to the 'sample' index\n",
    "    img = train_X[sample]\n",
    "\n",
    "    # The label of the image\n",
    "    label = train_y[sample]\n",
    "\n",
    "    # The image is stored as a rolled vector,\n",
    "    # we have to roll it back in a matrix\n",
    "    aimg = array(img).reshape(img_side, img_side)\n",
    "\n",
    "    # Open a subplot for each sample\n",
    "    subplot(1, num_samples, sample+1)\n",
    "\n",
    "    # The corresponding digit is the title of the plot\n",
    "    title(label)\n",
    "\n",
    "    # We use imshow to plot the matrix of pixels\n",
    "    imshow(aimg, interpolation = 'none',\n",
    "        aspect = 'auto', cmap = cm.binary)\n",
    "    axis(\"off\")\n",
    "\n",
    "show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Algorithme de K-medoïde\n",
    "## Distance Euclidienne\n",
    "Nous allons commencer par faire l'algorithme de k-medoïde avec la distance euclidienne pour MNIST. Dans le bloc suivant, il y aura le même algorithme, mais avec la similarité de pixel."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html?fbclid=IwAR2CjekTcfhnp19Txryk2u5c2w2Zoa9O1kEBR710Bcg8kPNuCXQWj0-_zm8\n",
    "# https://medium.com/@ali.soleymani.co/beyond-scikit-learn-is-it-time-to-retire-k-means-and-use-this-method-instead-b8eb9ca9079a\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "fp = kmedoids.fasterpam(euclidean_distances(X_principal), medoids=10, max_iter=100)\n",
    "\n",
    "print(\"FasterPAM with euclidean distance took: %.2f ms\" % ((time() - start)*1000))\n",
    "print(\"Loss with FasterPAM:\", fp.loss)\n",
    "homo = metrics.homogeneity_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "compl = metrics.completeness_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "v_meas = metrics.v_measure_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "ARI = metrics.adjusted_rand_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "AMI = metrics.adjusted_mutual_info_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "silhouette = metrics.silhouette_score(X_principal,train_y, metric='euclidean', sample_size=np.shape(X_principal)[0])\n",
    "\n",
    "print(82 * \"_\")\n",
    "print(\"\\ttime\\thomo\\tcompl\\tv_meas\\tARI\\tAMI\\tsilhouette\")\n",
    "print(f\"{(time() - start)*1000} ms {homo} {compl} {v_meas} {ARI} {AMI} {silhouette}\")\n",
    "print(82 * \"_\")\n",
    "\n",
    "# Commented this section because it takes way too much time, for very similar results, partly due to the deterministic nature of PCA\n",
    "# start = time()\n",
    "# pam = kmedoids.pam(diss, medoids=10, max_iter=100)\n",
    "# homo = metrics.homogeneity_score(labels_true=train_y, labels_pred=pam.labels)\n",
    "# compl = metrics.completeness_score(labels_true=train_y, labels_pred=pam.labels)\n",
    "# v_meas = metrics.v_measure_score(labels_true=train_y, labels_pred=pam.labels)\n",
    "# ARI = metrics.adjusted_rand_score(labels_true=train_y, labels_pred=pam.labels)\n",
    "# AMI = metrics.adjusted_mutual_info_score(labels_true=train_y, labels_pred=pam.labels)\n",
    "# silhouette = metrics.silhouette_score(X_principal,train_y, metric='euclidean', sample_size=np.shape(X_principal)[0])\n",
    "# print(\"Loss with PAM:\", pam.loss)\n",
    "# print(\"PAM took: %.2f ms\" % ((time() - start)*1000))\n",
    "# print(82 * \"_\")\n",
    "# print(\"\\ttime\\thomo\\tcompl\\tv_meas\\tARI\\tAMI\\tsilhouette\")\n",
    "# print(f\"{(time() - start)*1000} ms {homo} {compl} {v_meas} {ARI} {AMI} {silhouette}\")\n",
    "# print(82 * \"_\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarité de Pixel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "fp = kmedoids.fasterpam(get_pixel_similarity_matrix(X_std_pixel_train), medoids=10, max_iter=100)\n",
    "\n",
    "print(\"FasterPAM with pixel similarity took: %.2f ms\" % ((time() - start)*1000))\n",
    "print(\"Loss with FasterPAM:\", fp.loss)\n",
    "homo = metrics.homogeneity_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "compl = metrics.completeness_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "v_meas = metrics.v_measure_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "ARI = metrics.adjusted_rand_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "AMI = metrics.adjusted_mutual_info_score(labels_true=train_y, labels_pred=fp.labels)\n",
    "silhouette = metrics.silhouette_score(get_pixel_similarity_matrix(X_std_pixel_train),train_y, metric='precomputed', sample_size=np.shape(X_std_pixel_train)[0])\n",
    "\n",
    "print(82 * \"_\")\n",
    "print(\"\\ttime\\thomo\\tcompl\\tv_meas\\tARI\\tAMI\\tsilhouette\")\n",
    "print(f\"{(time() - start)*1000} ms {homo} {compl} {v_meas} {ARI} {AMI} {silhouette}\")\n",
    "print(82 * \"_\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "Voici les métriques collectées pour la distance euclidienne :\n",
    "__________________________________________________________________________________\n",
    "\ttime(s) homo    compl   v_meas  ARI     AMI     silhouette\n",
    "    620.091 0.2175  0.2234  0.2204  0.1427  0.2202  -0.0670\n",
    "__________________________________________________________________________________\n",
    "\n",
    "Et voici les métriques collectées pour la similarité de pixel :\n",
    "__________________________________________________________________________________\n",
    "\ttime(s) homo    compl   v_meas  ARI     AMI     silhouette\n",
    "\t1407.364 0.1017 0.1037  0.1027  0.0527  0.1024  -0.1337\n",
    "__________________________________________________________________________________\n",
    "\n",
    "Il est assez clair ici avec ces métriques, que la similarité de pixel est beaucoup plus lente, environ 2x plus lente et les groupes sont beaucoup moins homogènes. En général, les métriques sont beaucoup moins bonnes pour la similarité, on peut donc en conclure que ce n'est pas une bonne métrique pour l'algorithme des K-medoïde."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Partionnement Binaire\n",
    "## Distance Euclidienne\n",
    "Ici nous ferons la partition binaire avec la distance euclidienne pour MNIST"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prédictions en faisant la moyenne\n",
    "def agglomerative_clustering_predict(agglomerative_clustering, X):\n",
    "    average = list()\n",
    "    ith_clusters = list()\n",
    "    for i in range(agglomerative_clustering.n_clusters):\n",
    "        # On fouille dans les train_X pour tous les labels i\n",
    "        for j in range(len(agglomerative_clustering.labels_)):\n",
    "            if agglomerative_clustering.labels_[j]==i:\n",
    "                ith_clusters.append(X.iloc[j])\n",
    "        average.append(np.array(ith_clusters).mean(axis=1))\n",
    "    # np.stack makes this error: all input arrays must have the same shape\n",
    "    return np.argmin(np.stack(average), axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "# https://www.projectpro.io/recipes/do-agglomerative-clustering-in-python\n",
    "# https://www.geeksforgeeks.org/implementing-agglomerative-clustering-using-sklearn/\n",
    "\n",
    "# pca = PCA(n_components = 2)\n",
    "# # Training data set\n",
    "# X_principal = pca.fit_transform(X_std_train)\n",
    "# X_principal = pd.DataFrame(X_principal)\n",
    "# print(X_principal)\n",
    "#X_principal.columns = ['P1', 'P2']\n",
    "\n",
    "# # Testing data set\n",
    "# X_secondary = pca.fit_transform(X_normalized_test)\n",
    "# X_secondary = pd.DataFrame(X_secondary)\n",
    "# X_secondary.columns = ['P1', 'P2']\n",
    "\n",
    "# Dendogram requires normalized data, which is bad for mnist\n",
    "# plt.figure(figsize =(8, 8))\n",
    "# plt.title('Visualising the data')\n",
    "# Dendrogram = shc.dendrogram((shc.linkage(X_principal, method ='ward')))\n",
    "\n",
    "\n",
    "start = time()\n",
    "ac10 = AgglomerativeClustering(n_clusters = 10, affinity='euclidean')\n",
    "labels = pd.DataFrame(ac10.fit_predict(X_std_train))\n",
    "\n",
    "#vscore = v_measure_score(labels_true=train_y, labels_pred=labels.iloc[:, 0])\n",
    "\n",
    "#print(f\"V-measure score for agglomerative clustering with 10 clusters: {vscore}\")\n",
    "\n",
    "print(\"Agglomerative clustering with euclidean distance took: %.2f s\" % (time() - start))\n",
    "homo = metrics.homogeneity_score(labels_true=train_y, labels_pred=labels.iloc[:, 0])\n",
    "compl = metrics.completeness_score(labels_true=train_y, labels_pred=labels.iloc[:, 0])\n",
    "v_meas = v_measure_score(labels_true=train_y, labels_pred=labels.iloc[:, 0])\n",
    "ARI = metrics.adjusted_rand_score(labels_true=train_y, labels_pred=labels.iloc[:, 0])\n",
    "AMI = metrics.adjusted_mutual_info_score(labels_true=train_y, labels_pred=labels.iloc[:, 0])\n",
    "silhouette = metrics.silhouette_score(X_std_train,train_y, metric='euclidean', sample_size=np.shape(X_std_train)[0])\n",
    "# V-measure score for agglomerative clustering with 10 clusters: 0.5844426454187285\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarité de Pixel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "start = time()\n",
    "ac_pixel = AgglomerativeClustering(n_clusters = 10, affinity='precomputed', linkage='average')\n",
    "labels = ac_pixel.fit_predict(get_pixel_similarity_matrix(X_std_pixel_train))\n",
    "print(f\"V-measure score for agglomerative clustering with 10 clusters: {v_measure_score(labels_true=train_y, labels_pred=labels)}\")\n",
    "print(\"Agglomerative clustering with pixel similarity took: %.2f s\" % (time() - start))\n",
    "# Agglomerative clustering with pixel similarity took: 12135.04 s (202.25 minutes / 3.37 hours)\n",
    "# V-measure score for agglomerative clustering with 10 clusters: 0.1076184112661532"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "Puisque nous avons accès aux étiquettes pour les données, nous pouvons utiliser le v_measure_score pour évaluer la précision de l'algorithme.\n",
    "Voici les résultats pour la distance euclidienne :\n",
    "__V-measure score for agglomerative clustering with 10 clusters : 0.5844426454187285__\n",
    "\n",
    "Et les résultats pour la similarité de Pixel :\n",
    "__Agglomerative clustering with pixel similarity took: 12135.04 s (202.25 minutes / 3.37 hours)__\n",
    "__V-measure score for agglomerative clustering with 10 clusters: 0.1076184112661532__\n",
    "\n",
    "Le résultat est assez évident que la similarité de pixel n'est pas du tout capable de faire une séparation claire. C'est à peine meilleur que de tirer au hasard, ce qui à vrai dire, n'est pas utile. En termes de temps, l'algorithme avec la similarité prend plus de temps que celle avec la distance euclidienne. Pour l'algorithme de partionnement, la similarité est pire que la distance euclidienne en tout points."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCoA\n",
    "## Distance Euclidienne\n",
    "Ici nous ferons le PCoA avec la distance euclidienne pour MNIST"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "pcoa = KernelPCA(n_components=1, kernel='euclidean')\n",
    "\n",
    "# Euclidean distance\n",
    "start = time()\n",
    "pcoa_euclidean_train = pcoa.fit_transform(-.5 * euclidean_distances(X_std_train) ** 2) #-.5*D**2 est crucial!!!\n",
    "print(\"PCoA training with euclidean distance took: %.2f s\" % (time() - start))\n",
    "start = time()\n",
    "pcoa_euclidean_test = pcoa.transform(-.5 * euclidean_distances(X_std_test) ** 2) #-.5*D**2 est crucial!!!\n",
    "print(\"PCoA testing with euclidean distance took: %.2f s\" % (time() - start))\n",
    "\n",
    "\n",
    "# Euclidean distance\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(211)\n",
    "ax.set_title('PCoA sur train')\n",
    "ax.scatter(pcoa_euclidean_train, np.zeros_like(pcoa_euclidean_train))\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.set_title('PCoA sur test')\n",
    "ax.scatter(pcoa_euclidean_test, np.zeros_like(pcoa_euclidean_test))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarité de Pixel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "pcoa = KernelPCA(n_components=1, kernel='precomputed')\n",
    "\n",
    "# Pixel similarity\n",
    "start = time()\n",
    "pcoa_pixel_train = pcoa.fit_transform(-.5 * get_pixel_similarity_matrix(X_std_pixel_train) ** 2) #-.5*D**2 est crucial!!!\n",
    "print(\"PCoA training with pixel similarity took: %.2f s\" % (time() - start))\n",
    "# PCoA training with pixel similarity took: 2932.46 s (49 minutes)\n",
    "\n",
    "start = time()\n",
    "pcoa_pixel_test = pcoa.transform(-.5 * get_pixel_similarity_matrix(X_std_pixel_test, X_std_pixel_train) ** 2) #-.5*D**2 est crucial!!!\n",
    "print(\"PCoA testing with pixel similarity took: %.2f s\" % (time() - start))\n",
    "# PCoA training with pixel similarity took: 2535.06 s\n",
    "# PCoA testing with pixel similarity took: 343.85 s\n",
    "\n",
    "# Pixel similarity\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(211)\n",
    "ax.set_title('PCoA sur train')\n",
    "ax.scatter(pcoa_pixel_train, np.zeros_like(pcoa_pixel_train))\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.set_title('PCoA sur test')\n",
    "ax.scatter(pcoa_pixel_test, np.zeros_like(pcoa_pixel_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "Voici le résultat que nous avons obtenu pour la distance euclidienne :\n",
    "![Euclidean Distance](Rapport/pcoa_euclidiean.png)\n",
    "\n",
    "Et voici le résultat de la similarité de pixel :\n",
    "![Similarité Pixel](Rapport/pcoa_pixel_test.png)\n",
    "__PCoA training with pixel similarity took: 2535.06 s__\n",
    "__PCoA testing with pixel similarity took: 343.85 s__\n",
    "__Total: 2,878.91 s (48 m)__\n",
    "\n",
    "Puisque le PCoA est une transformation sur le plan à 1 dimension, et que la similarité de pixel est déjà réduite à une dimension, il aurait été logique que la transformation soit plus rapide. Cependant, cela a pris plus de temps, car le calcul de la similarité de pixel inclus une réduction de dimensionalité dans son calcul, ce qui prend plus de temps que la distance euclidienne. Finalement, la similarité de pixel est plus lente que la distance euclidienne pour PCoA."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KNN\n",
    "## Distance Euclidienne\n",
    "Ici nous ferons le KNN avec la distance euclidienne pour MNIST. Après quelques tests aléatoire, nous avons trouvé que 5 voisins était un bon hyperparamètre pour l'algorithme de KNN dans le cas de MNIST."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2, algorithm='auto')\n",
    "knn.fit(train_X, train_y)\n",
    "print(\"KNN training with euclidean distance took: %.2f s\" % (time() - start))\n",
    "\n",
    "start = time()\n",
    "predictions = knn.predict(test_X)\n",
    "print(\"KNN predictions with euclidean distance took: %.2f s\" % (time() - start))\n",
    "\n",
    "acc = accuracy_score(y_true=test_y, y_pred=predictions)\n",
    "print(f\"Accuracy for KNN algorithm with euclidean distance: {acc}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarité de Pixel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='precomputed', algorithm='auto')\n",
    "knn.fit(get_pixel_similarity_matrix(X_std_pixel_train), train_y)\n",
    "print(\"KNN training with pixel similarity took: %.2f s\" % (time() - start))\n",
    "# KNN training with pixel similarity took: 1261.69 s (21 minutes)\n",
    "\n",
    "start = time()\n",
    "predictions = knn.predict(get_pixel_similarity_matrix(X_std_pixel_test, X_std_pixel_train))\n",
    "print(\"KNN predictions with pixel similarity took: %.2f s\" % (time() - start))\n",
    "acc = accuracy_score(y_true=test_y, y_pred=predictions)\n",
    "print(f\"Accuracy for KNN algorithm with pixel similarity: {acc}\")\n",
    "# KNN training with pixel similarity took: 4880.94 s\n",
    "# KNN predictions with pixel similarity took: 726.11 s\n",
    "# Accuracy for KNN algorithm with pixel similarity: 0.1904"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "Voici nos métriques pour la distance euclidienne:\n",
    "\n",
    "Voici nos métriques pour la similarité de pixel:\n",
    "__KNN training with pixel similarity took: 4880.94 s__\n",
    "__KNN predictions with pixel similarity took: 726.11 s__\n",
    "__Accuracy for KNN algorithm with pixel similarity: 0.1904__\n",
    "\n",
    "Ici la différence est très flagrante. La précision de 19 % pour la similarité est très mauvais comparé à celui de la distance euclidienne de 98.6 %. La différence en temps est très énorme aussi. La similarité prend plusieurs minutes sur le jeu de données complet, tandis que la distance euclidienne retourne un résultat pratiquement instantanément. Pour terminer, la similarité de pixel est moins bonne que la distance euclidienne pour l'algorithme de KNN."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Isomap\n",
    "## Distance Euclidienne\n",
    "Ici nous ferons l’Isomap avec la distance euclidienne pour MNIST. Pour cet algorithme, nous avons aussi testé quelques valeurs pour le nombre de voisins et nous avions trouvé que 5 donnait un bon résultat."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "isomap = Isomap(n_components=2, n_neighbors=5, metric='minkowski', p=2)\n",
    "\n",
    "start = time()\n",
    "X_transformed_isomap = isomap.fit_transform(X_std_train)\n",
    "\n",
    "print(\"Isomap took: %.2f s\" % (time() - start))\n",
    "\n",
    "fig = plt.figure(figsize=(24, 24))\n",
    "ax = fig.add_subplot(211)\n",
    "ax.set_title('Isomap euclidien sur MNIST')\n",
    "ax.scatter(X_transformed_isomap, np.zeros_like(X_transformed_isomap))\n",
    "\n",
    "# Isomap_5n took: 6431123.26 ms (6,431.12326 s) (107.19 m) (1.79 h)\n",
    "# UserWarning: The number of connected components of the neighbors graph is 37 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
    "# self._fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarité de Pixel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "isomap = Isomap(n_components=1, n_neighbors=10, metric='precomputed')\n",
    "start = time()\n",
    "X_transformed_isomap = isomap.fit_transform(sparse.lil_matrix(get_pixel_similarity_matrix(X_std_pixel_train)))\n",
    "\n",
    "print(\"Isomap took: %.2f s\" % (time() - start))\n",
    "\n",
    "fig = plt.figure(figsize=(24, 24))\n",
    "ax = fig.add_subplot(211)\n",
    "ax.set_title('Isomap pixel sur MNIST')\n",
    "ax.scatter(X_transformed_isomap, np.zeros_like(X_transformed_isomap))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Voici nos résultats pour Isomap à 5 voisins et la distance euclidienne :\n",
    "__Isomap_5n took: 6431123.26 ms (6,431.12326 s) (107.19 m) (1.79 h)__\n",
    "Voici nos résultats pour Isomap à 5 voisins et la similarité de pixel :"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Résumé\n",
    "Lors de nos observations pour les différences entre les algorithmes, nous avons remarqué que la similarité choisie était pire en tout points à la distance euclidienne. À chaque fois que nous l'avons évalué, elle était plus lente et beaucoup moins précise, ce qui nous mènent donc à la conclusion que la similarité de pixel est pire que la distance euclidienne pour l'évaluation des images. Nous étions conscients de la perte d'information lors de notre compression de l'image à une seule dimension, mais nous ne pensions pas que cette transformation donnerait des si mauvais résultats. Visiblement, l'addition des pixels ne permet pas de différencier les images assez bien, surement dû au fait que les images peuvent contenir un nombre de pixels assez différent dans la même catégorie, par exemple un 1 qui est écrit avec la barre en haut et en bas à un nombre complètement différent de pixel qu'un 1 qui est écrit seulement avec la barre du milieu. Cela doit causer beaucoup trop de variance dans les catégories, ce qui doit surement confondre les catégories ensemble et nuire à la machine d'apprendre des catégories distinctes. Nous aurions surement pu binariser nos données et cela nous aurait peut-être un peu aidé à mieux distinguer les groupes. En conclusion, notre similarité choisie pour MNIST n'était pas très bonne et ne devrait pas être utilisé pour ce contexte, et encore moins pour les images en couleur si nous ne modifions rien aux calculs et à la réduction de dimensionalité."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}